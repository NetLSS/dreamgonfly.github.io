<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>aws on Dreamgonfly&#39;s blog</title>
    <link>https://dreamgonfly.github.io/tags/aws/</link>
    <description>Recent content in aws on Dreamgonfly&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>© {year}</copyright>
    <lastBuildDate>Fri, 19 Jan 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://dreamgonfly.github.io/tags/aws/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>AWS Lambda로 PyTorch 모델 서빙하기</title>
      <link>https://dreamgonfly.github.io/blog/pytorch-on-aws-lambda/</link>
      <pubDate>Fri, 19 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://dreamgonfly.github.io/blog/pytorch-on-aws-lambda/</guid>
      <description>AWS Lambda로 PyTorch 모델 서빙하기 AWS Lambda는 서버 관리의 부담을 없애주는 서버 리스 컴퓨팅(Serverless computing) 서비스입니다. Lambda를 한마디로 설명하면 이벤트가 발생했을 때만 서버가 떠서 코드를 실행하는 이벤트 기반 클라우드 플랫폼입니다. Lambda는 코드가 실행된 시간에 대해서만 비용을 내는 효율성과, 이벤트가 갑자기 많이 발생해도 병렬처리가 가능한 확장성 덕분에 각광받고 있습니다.
이 글에서는 Lambda 위에 PyTorch 모델을 업로드하여 API로 서비스하는 방법을 공유하겠습니다. 이 글은 step-by-step으로 구성되어 있습니다. 배포 준비를 위해 Docker를 설치하고 PyTorch 라이브러리와 모델을 압축파일로 만들고 Lambda 위에 올린 뒤 API를 배포하는 것까지 차근차근 따라가 보겠습니다.</description>
    </item>
    
  </channel>
</rss>